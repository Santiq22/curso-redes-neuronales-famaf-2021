{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refs.\n",
    "\n",
    "1. Introduction to the theory of neuronal computation, Hertz, Krogh, Palmer (1991)\n",
    "\n",
    "2. https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/\n",
    "\n",
    "3. https://towardsdatascience.com/perceptron-explanation-implementation-and-a-visual-example-3c8e76b4e2d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teoría\n",
    "\n",
    "### Múltiples capas y neuronas de salida\n",
    "\n",
    "Consideramos un perceptrón de $m+1$ capas y de $n_l$ neuronas en la $l$-ésima capa, siendo $n_0$ el número de neuronas en la capa de entrada y $n_m$ el número de neuronas en la capa de salida.\n",
    "\n",
    "La componente $V_{i}^{(l)}\\in\\mathbb{R}$ representa el estado de la $i$-ésima neurona en la $l$-ésima capa,\n",
    "la componente $w_{ji}^{(l)}\\in\\mathbb{R}$ el peso sináptico saliendo de la $i$-ésima neurona en la $(l-1)$-ésima capa y entrando a la $j$-ésima neurona de la $l$-ésima capa.\n",
    "De esta manera, el estado $V_{j}^{(l+1)}$ de la $j$-ésima neurona de la $(l+1)$-ésima capa viene determinado por\n",
    "\n",
    "$$ V_{j}^{(l+1)} = g(h_{j}^{l+1}) \\;\\;\\; (1)$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$ h_{j}^{(l+1)} := \\sum_i w_{ji}^{(l+1)}V_{i}^{(l)} $$\n",
    "\n",
    "Asuminos que en cada capa $l<m$ existe una neurona de estado $V_{0}^{(l)}=1$ con el fin de implementar el truquito que nos permite remplazar el umbral en cada neurona por un peso sináptico.\n",
    "\n",
    "Notar que, con excepción de las neuronas de entrada en la capa $l=0$, todas las neuronas usan la misma función activación $g$.\n",
    "Existen muchas funciones de activación, pero nos enfocaremos en usar\n",
    "\n",
    "\\begin{eqnarray}\n",
    "g(x)\n",
    "&:=& \\tanh(\\beta x) \\\\\n",
    "&=& \\frac{e^{\\beta x}-e^{-\\beta x}}{e^{\\beta x}+e^{-\\beta x}} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "cuya derivada convenientemente satisface\n",
    "\n",
    "$$ g' = \\beta(1-g^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento\n",
    "\n",
    "Consideraremos una serie de datos de $\\mu=1,...,p$ puntos, de entradas $\\xi_{\\mu i}$ y salidas $\\zeta_{\\mu j}$ en $\\mathbb{R}$.\n",
    "Es decir, $x_{\\mu i}$ representa el valor de entrada de la $i$-ésima neurona en la capa $l=0$ y $\\zeta_{\\mu j}$ el valor pretendido de salida en la $j$-ésima neurona de la capa $l=m$.\n",
    "\n",
    "El objetivo es entrenar los pesos sinápticos representados por $w$, de manera que la salida \n",
    "\n",
    "$$ V_{\\mu j}^{(m)} =: O_{\\mu j} \\approx \\zeta_{\\mu j}\\;\\;\\; (2) $$ \n",
    "\n",
    "cuando la entrada es\n",
    "\n",
    "$$ V_{\\mu i}^{(0)} = \\xi_{\\mu i} \\;\\;\\; (3) $$\n",
    "\n",
    "para todo $i=1,...,n_0$ y $j=1,...,n_m$ y punto $\\mu=1,...,p$.\n",
    "Aquí, $V_{\\mu j}^{(m)}$ es el valor que adopta la $j$-ésima neurona en la capa $m$ cuando la red es expuesta a la $\\mu$-ésima entrada.\n",
    "\n",
    "Formalmente, buscaremos minimizar el error cuadrático\n",
    "\n",
    "\\begin{eqnarray}\n",
    "E := \\sum_{\\mu j} \\frac{1}{2}(\\zeta_{\\mu j}-O_{\\mu j})^2 $$\n",
    "\\end{eqnarray}\n",
    "\n",
    "con respecto a $w$.\n",
    "Aquí, debemos recordar que $V_{\\mu j}^{(m)}$ depende de los pesos sinápticos $w$ y de las entradas $x$ (ver Ecs. (1,2,3)).\n",
    "\n",
    "Como algoritmo de minimización, utilizaremos una versión del método de descenso por el gradiente llamado **back-propagation**.\n",
    "Combiene simplificar notación y definir el operador derivada parcial $\\partial_x$ por $\\partial_x f := \\frac{\\partial }{\\partial x}f$ para cualquier funcion $f:\\mathbb{R}\\to \\mathbb{R}$.\n",
    "La idea nace de calcular el gradiente de $e$ respecto de $w$, capa por capa\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\partial_{w_{rs}^{(m)}}E\n",
    "&=&\\sum_{\\mu j} \\frac{1}{2}\\partial_{w_{rs}^{(m)}}(\\zeta_{\\mu j}-O_{\\mu j})^2\\\\\n",
    "&=&\\sum_{\\mu j} \\frac{1}{2}\\partial_{w_{rs}^{(m)}}(\\zeta_{\\mu j}-V_{\\mu j}^{(m)})^2\\\\\n",
    "&=&\\sum_{\\mu j} (\\zeta_{\\mu j}-V_{\\mu j}^{(m)})\\partial_{w_{rs}^{(m)}}(\\zeta_{\\mu j}-V_{\\mu j}^{(m)})\\\\\n",
    "&=&-\\sum_{\\mu j}(\\zeta_{\\mu j}-V_{\\mu j}^{(m)})\\partial_{w_{rs}^{(m)}}V_{\\mu j}^{(m)}\\\\\n",
    "&=&\\sum_{\\mu j}(V_{\\mu j}^{(m)}-\\zeta_{\\mu j})\\partial_{w_{rs}^{(m)}}g(h_{\\mu j}^{(m)})\\\\\n",
    "&=&\\sum_{\\mu j}(V_{\\mu j}^{(m)}-\\zeta_{\\mu j})g'(h_{\\mu j}^{(m)})\\partial_{w_{rs}^{(m)}}h_{\\mu j}^{(m)}\\\\\n",
    "&=&\\sum_{\\mu j}(V_{\\mu j}^{(m)}-\\zeta_{\\mu j})g'(h_{\\mu j}^{(m)})\\partial_{w_{rs}^{(m)}}\\sum_i w_{ji}^{(m)}V_{\\mu i}^{(m-1)}\\\\\n",
    "&=&\\sum_{\\mu ji}(V_{\\mu j}^{(m)}-\\zeta_{\\mu j})g'(h_{\\mu j}^{(m)})\\big(\\delta_{mm}\\delta_{rj}\\delta_{si}V_{\\mu i}^{(m-1)}+w_{ji}^{(m)}\\partial_{w_{rh}^{(m)}}V_{\\mu i}^{(m-1)}\\big)\\\\\n",
    "&=&\\sum_{\\mu ji}(V_{\\mu j}^{(m)}-\\zeta_{\\mu j})g'(h_{\\mu j}^{(m)})\\big(\\delta_{rj}\\delta_{si}V_{\\mu i}^{(m-1)}+w_{ji}^{(m)}0\\big)\\\\\n",
    "&=&\\sum_{\\mu }(V_{\\mu r}^{(m)}-\\zeta_{\\mu r})g'(h_{\\mu r}^{(m)})V_{\\mu s}^{(m-1)}\\\\\n",
    "&=:&\\sum_{\\mu }(V_{\\mu r}^{(m)}-\\zeta_{\\mu r})g'(h_{\\mu r}^{(m)})V_{\\mu s}^{(m-1)}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "donde usamos que $\\partial_{w_{rs}^{(m)}}V_{i}^{(m-1)}=0$ porque $V_{i}^{(l)}$ no depende de $w_{rs}^{(f)}$ para todo $f>l$ como puede deducirse de\n",
    "\n",
    "<img src=\"fig1.png\" width=\"400\">\n",
    "\n",
    "Luego, introduciendo\n",
    "\n",
    "$$ d^{(m)}_{\\mu r} := (O_{\\mu r}-\\zeta_{\\mu r})g'(h_{\\mu r}^{(m)}) $$\n",
    "\n",
    "reescribimos el anterior resultado como\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\partial_{w_{rs}^{(m)}}E\n",
    "&=&\\sum_{\\mu }d^{(m)}_{\\mu r}V_{\\mu s}^{(m-1)}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Más generalmente, \n",
    "\\begin{eqnarray}\n",
    "\\partial_{w_{rs}^{(m-p)}}E\n",
    "&=&\\sum_{\\mu j}(V_{\\mu j}^{(m)}-\\zeta_{\\mu j})g'(h_{\\mu j}^{(m)})\\partial_{w_{rs}^{(m-p)}}h_{\\mu j}^{(m)}\\\\\n",
    "&=&\\sum_{\\mu j}d^{(m)}_{\\mu r}\\partial_{w_{rs}^{(m-p)}}\\sum_i w_{ji}^{(m)}V_{\\mu i}^{(m-1)}\\\\\n",
    "&=&\\sum_{\\mu ji}d^{(m)}_{\\mu r}w_{ji}^{(m)}\\partial_{w_{rs}^{(m-p)}}V_{\\mu i}^{(m-1)}\\\\\n",
    "&=&\\sum_{\\mu ji}d^{(m)}_{\\mu r}w_{ji}^{(m)}\\partial_{w_{rs}^{(m-p)}}g(h_{\\mu i}^{(m-1)})\\\\\n",
    "&=&\\sum_{\\mu ji}d^{(m)}_{\\mu r}w_{ji}^{(m)}g'(h_{\\mu i}^{(m-1)})\\partial_{w_{rs}^{(m-p)}}h_{\\mu i}^{(m-1)}\\\\\n",
    "&=&\\sum_{\\mu i}g'(h_{\\mu i}^{(m-1)})\\big(\\sum_j d^{(m)}_{\\mu r}w_{ji}^{(m)}\\big)\\partial_{w_{rs}^{(m-p)}}s_{\\mu i}^{(m-1)}\\\\\n",
    "&=&\\sum_{\\mu i}d^{(m-1)}_{\\mu i}\\partial_{w_{rs}^{(m-p)}}h_{\\mu i}^{(m-1)}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "donde\n",
    "\n",
    "$$ d^{(m-1)}_{\\mu i} := g'(h_{\\mu i}^{(m-1)})\\sum_j d^{(m)}_{\\mu r}w_{ji}^{(m)} $$\n",
    "\n",
    "Esto sugiere proceder por inducción.\n",
    "Sabemos que la hipótesis inductiva\n",
    "\n",
    "$$ \\partial_{w_{rs}^{(m-p)}}e = \\sum_{\\mu i}d^{(m-f)}_{\\mu i}\\partial_{w_{rs}^{(m-p)}}h_{ki}^{(m-f)} $$\n",
    "\n",
    "vale para $f=0$.\n",
    "Asumiendo que vale para $f<q$, se tiene\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\partial_{w_{rs}^{(m-q)}}E\n",
    "&=&\\sum_{\\mu i}d^{(m-f)}_{\\mu i}\\partial_{w_{rs}^{(m-q)}}h_{\\mu i}^{(m-f)}\\\\\n",
    "&=&\\sum_{\\mu i}d^{(m-f)}_{\\mu i}\\partial_{w_{rs}^{(m-q)}}\\sum_j w_{ij}^{(m-f)}V_{\\mu j}^{(m-f-1)}\\\\\n",
    "&=&\\sum_{\\mu ij}d^{(m-f)}_{\\mu i}w_{ij}^{(m-f)}\\partial_{w_{rs}^{(m-q)}}V_{\\mu j}^{(m-f-1)}\\\\\n",
    "&=&\\sum_{\\mu ij}d^{(m-f)}_{\\mu i}w_{ij}^{(m-f)}\\partial_{w_{rs}^{(m-q)}}g(h_{\\mu j}^{(m-f-1)})\\\\\n",
    "&=&\\sum_{\\mu ij}d^{(m-f)}_{\\mu i}w_{ij}^{(m-f)}g'(h_{\\mu j}^{(m-f-1)})\\partial_{w_{rs}^{(m-q)}}h_{\\mu j}^{(m-f-1)}\\\\\n",
    "&=&\\sum_{\\mu j}g'(h_{\\mu j}^{(m-f-1)})\\big(\\sum_i d^{(m-f)}_{\\mu i}w_{ij}^{(m-f)}\\big)\\partial_{w_{rs}^{(m-q)}}h_{\\mu j}^{(m-f-1)}\\\\\n",
    "&=&\\sum_{\\mu j}d^{(m-f-1)}_{\\mu j}\\partial_{w_{rs}^{(m-q)}}h_{\\mu j}^{(m-f-1)}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "i.e., vemos que vale para $f+1$ cuando \n",
    "\n",
    "$$ d^{(m-f-1)}_{\\mu i} := g'(h_{\\mu i}^{(m-f-1)})\\sum_j d^{(m-f)}_{\\mu j}w_{ji}^{(m-f)} \\;\\;\\; (4)$$\n",
    "\n",
    "Finalmente, cuando $f=q$ se tiene\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\partial_{w_{rs}^{(m-q)}}E\n",
    "&=&\\sum_{\\mu i}d^{(m-q)}_{\\mu i}\\partial_{w_{rs}^{(m-q)}}\\sum_j w_{ij}^{(m-q)}V_{\\mu j}^{(m-q-1)}\\\\\n",
    "&=&\\sum_{\\mu ij}d^{(m-q)}_{\\mu i}\\delta_{ri}\\delta_{sj}V_{\\mu j}^{(m-q-1)}\\\\\n",
    "&=&\\sum_{\\mu }d^{(m-q)}_{\\mu r}V_{\\mu s}^{(m-q-1)}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "En otras palabras, hemos encontrado expresión cerrada (i.e., en donde ya no hay derivadas) para cada componente del gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo\n",
    "\n",
    "El principal objetivo es minimizar el error $e$ usando el algoritmo de descenso por el gradiente, iterando\n",
    "\n",
    "$$ w^{(l)}_{ji}(t+1) = w^{(l)}_{ji}(t) - \\eta \\partial_{w^{(l)}_{ji}}E(t) $$\n",
    "\n",
    "sobre $t$ para todo $l$ y $ji$, partiendo de una condición inicial aleatoria $w^{(l)}_{ji}(t=0) \\sim \\mathcal{N}$ donde $\\mathcal{N}$ representa una distribución normal de media 0 y varianza 1.\n",
    "\n",
    "Para calcular los $\\partial_{w^{(l)}_{ji}}e$ necesitamos calcular los $V^{(l)}_{\\mu i}$ y $d^{(l)}_{\\mu i}$.\n",
    "Ambos requieren del cálculo de los $h^{(l)}_{\\mu i}$.\n",
    "Observando la Ec. (4), nos damos cuenta que es necesario calcular primero los $d^{(m)}_{\\mu i}$, luego los $d^{(m-1)}_{\\mu i}$ y así hasta poder calcular los $d^{(1)}_{\\mu i}$.\n",
    "\n",
    "Luego de inicializar\n",
    "\n",
    "$$ V^{(0)}_{\\mu i} = \\xi_{\\mu i} $$\n",
    "\n",
    "el cálculo se puede realizar en dos fases. \n",
    "Primero la fase **forward**, en donde calculamos los $V^{(l)}_{\\mu i}$ y $h^{(l)}_{\\mu i}$ capa por capa en orde creciente en $l$.\n",
    "Más precisamente\n",
    "\n",
    "   \n",
    "1. Iterando sobre $l=1,2,...,m$, calculamos para cada $l$:\n",
    "\n",
    "    i. $h^{(l)}_{\\mu i} = \\sum_j w^{(l)}_{ij} V^{(l-1)}_{\\mu j}$\n",
    "    \n",
    "    ii. $V^{(l)}_{\\mu i} = g(h^{(l)}_{\\mu i})$\n",
    "\n",
    "Luego la fase **backwards**, en donde calculamos los $d^{(l)}_{ki}$, $\\partial_{w^{(l)}_{ji}}e$ y actualizamos los $w^{(l)}_{ji}$ en orden decreciente en $l$.\n",
    "Más precisamente\n",
    "\n",
    "1. Calculamos\n",
    "\n",
    "    i. $ d_{\\mu i}^{(m)} = g'(h^{(m)}_{\\mu i})(V_{\\mu i}^{(m)}-\\zeta_{\\mu i}) $\n",
    "\n",
    "    y actualizamos\n",
    "\n",
    "    ii. $ w^{(m)}_{ji} -= \\eta \\sum_{\\mu} d^{(m)}_{\\mu j} V^{(m-1)}_{\\mu i} $\n",
    "\n",
    "2. Luego, iterando sobre $l=m-1,m-2,...,1$, calculamos para cada $l$:\n",
    "\n",
    "    i. $ d_{\\mu i}^{(l)} = g'(h^{(l)}_{\\mu i}) \\sum_j d^{(l+1)}_{\\mu j} w^{(l+1)}_{ji} $\n",
    "    \n",
    "    y actualizamos\n",
    "    \n",
    "    ii. $ w^{(l)}_{ji} -= \\sum_{\\mu} d^{(l)}_{\\mu j} V^{(l-1)}_{\\mu i} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minipráctico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Implemente un perceptron de $m=3$ capas, una de entrada de $n_0=2$ neuronas, una oculta de $n_1=2$ neuronas y una de salida de $n_2=1$ neurona, utilizando $g(x) = \\tanh(\\beta x)$ como función activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=1.0\n",
    "def g(x):\n",
    "    return np.tanh(beta*x)\n",
    "def dg(x):\n",
    "    return beta*(1.0-g(x)**2)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self,n,x,z,epocas=100,eta=0.01,g=g,dg=dg):\n",
    "        \"\"\"\n",
    "        n: N^(1+m) lista de tamaños de capas (sin incluir neuronas truquito)\n",
    "        x: R^(q,n[0])\n",
    "        z: R^(q,n[m])\n",
    "        epocas: N\n",
    "        beta: R\n",
    "        eta: R\n",
    "        \"\"\"\n",
    "        m=len(n)-1 # n[0],n[1],...,n[m]\n",
    "        p,_=x.shape\n",
    "        self.n=n\n",
    "        self.m=m        \n",
    "        self.beta=beta\n",
    "        V = [None]*(1+m)\n",
    "        h = [None]*(1+m)\n",
    "        d = [None]*(1+m)\n",
    "        w = [None]*(1+m)\n",
    "        # Inicializamos pesos. Las primeras capas tienen en cuenta neuronas truquito. La ultima no.\n",
    "        for l in range(1,m):\n",
    "            w[l]=np.random.normal(size=(n[l]+1,n[l-1]+1))\n",
    "        w[m]=np.random.normal(size=(n[m],n[m-1]+1))\n",
    "        # Inicializamos activaciones, preactivaciones y diferencias\n",
    "        V[0] = np.ones((p,n[0]+1)) # Inicializamos y agregamos truquito a a^0_ki\n",
    "        V[0][:,1:] = x[:,:]\n",
    "        # Entrenamos\n",
    "        self.list_E=[]\n",
    "        for t in range(epocas):\n",
    "            # Forward\n",
    "            for l in range(1,m+1):\n",
    "                h[l] = np.tensordot(V[l-1],w[l],axes=([1],[1]))\n",
    "                V[l] = np.vectorize(g)(h[l])\n",
    "            # Calculamos error cuadratico y reportamos\n",
    "            E = V[m]-z.reshape(V[m].shape)\n",
    "            self.list_E.append(np.dot(E.flatten(),E.flatten()))\n",
    "            # Backward\n",
    "            d[m] = np.vectorize(dg)(h[m])*E\n",
    "            w[m] -= eta*np.tensordot(d[m],V[m-1],axes=([0],[0]))\n",
    "            for l in range(m-1,0,-1):\n",
    "                d[l] = np.vectorize(dg)(h[l])*np.tensordot(d[l+1],w[l+1],axes=([1],[0]))\n",
    "                w[l] -= eta*np.tensordot(d[l],V[l-1],axes=([0],[0]))\n",
    "        # grabamos los pesos como miembro de la clase\n",
    "        self.w=w\n",
    "    def __call__(self,x):\n",
    "        n=self.n\n",
    "        m=self.m\n",
    "        w=self.w\n",
    "        V = [None]*(1+m)\n",
    "        h = [None]*(1+m)\n",
    "        V[0] = np.ones(n[0]+1)\n",
    "        V[0][1:] = x\n",
    "        for l in range(1,m+1):\n",
    "            h[l] = np.dot(w[l],V[l-1])\n",
    "            V[l] = np.vectorize(g)(h[l])\n",
    "        return V[m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Usar `scikit-learn.datasets.make_classification` para crear un dataset para clasificación con:\n",
    "\n",
    "- 2 características (features)\n",
    "- 2 clases\n",
    "- 100 muestras\n",
    "- sin redundancia\n",
    "- 1 grupo (cluster) por clase\n",
    "\n",
    "Grafique el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "n0 = 2  # n[0]\n",
    "p = 100 # mu=0,1,...,p-1 donde p = número de muestras.\n",
    "n_classes = 2 \n",
    "x,y = make_classification(\n",
    "    n_features=n0,\n",
    "    n_classes=n_classes,\n",
    "    n_samples=p,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1\n",
    ")\n",
    "\n",
    "print(\"x.shape=\",x.shape,\"y.shape=\",y.shape,sep=\"\")\n",
    "\n",
    "color = {0:'red',1:'blue',2:'green',3:'cyan'}\n",
    "for k in range(x.shape[0]):\n",
    "    plt.scatter([x[k,0]],[x[k,1]],c=color[y[k]])\n",
    "plt.title(\"datos\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Entrene el perceptron en el dataset generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron([2,2,1],x,y)\n",
    "plt.plot(range(len(p.list_E)),p.list_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Grafique el resultado del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploteamos el resultado del entrenamiento\n",
    "def heaviside(x):\n",
    "    if x>0.5:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "num_error = 0\n",
    "for k in range(x.shape[0]):\n",
    "    y_pred=p(x[k,:])[0]\n",
    "    y_pred_bin = heaviside(y_pred)\n",
    "    if y[k]!=y_pred_bin:\n",
    "        num_error += 1\n",
    "    #print(\"k=\",k,\" y=\",y[k],\" y_pred_bin=\",y_pred_bin,\" y_pred=\",y_pred,sep=\"\")\n",
    "    c_pred = color[y_pred_bin]\n",
    "    c_true = color[y[k]]\n",
    "    plt.scatter([x[k,0]],[x[k,1]],color=c_pred,marker='.',edgecolors=c_true,linewidth=1,s=250)\n",
    "print(\"num_error=\",num_error,sep=\"\")\n",
    "    \n",
    "xmin = np.min(x[:,0])\n",
    "xmax = np.max(x[:,0])\n",
    "xs = np.linspace(xmin,xmax,100)\n",
    "ymax = np.max(x[:,1])\n",
    "ymin = np.min(x[:,1])\n",
    "plt.title(\"prediccion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** El problema de la compuerta **XOR**.\n",
    "\n",
    "La compuerta XOR viene dada por\n",
    "\n",
    "    0 0 -> 0\n",
    "    0 1 -> 1\n",
    "    1 0 -> 1    \n",
    "    1 1 -> 0\n",
    "    \n",
    "Muestre que un perceptrón simple no puede aprender la compuerta XOR, mientras que un perceptrón con sólo una capa oculta de dos neuronas (i.e. una capa de entrada de dos neuronas, una oculta de dos neuronas y una de salida de 1 neurona) si puede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos los datos\n",
    "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,1,1,0])\n",
    "for k in range(x.shape[0]):\n",
    "    plt.scatter([x[k,0]],[x[k,1]],c=color[y[k]])\n",
    "plt.title(\"datos\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el perceptron simple\n",
    "p_simple = Perceptron([2,1],x,y,epocas=4000)\n",
    "plt.plot(range(len(p_simple.list_E)),p_simple.list_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(x.shape[0]):\n",
    "    y_pred=p_simple(x[k,:])[0]\n",
    "    y_pred_bin = heaviside(y_pred)\n",
    "    print(\"x=\",x[k,:],\" y=\",y[k],\" y_pred_bin=\",y_pred_bin,\" y_pred=\",y_pred,sep=\"\")\n",
    "    c_pred = color[y_pred_bin]\n",
    "    c_true = color[y[k]]\n",
    "    plt.scatter([x[k,0]],[x[k,1]],color=c_pred,marker='.',edgecolors=c_true,linewidth=1,s=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el perceptron con una capa oculta.\n",
    "p = Perceptron([2,2,1],x,y,epocas=4000)\n",
    "plt.plot(range(len(p.list_E)),p.list_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(x.shape[0]):\n",
    "    y_pred=p(x[k,:])[0]\n",
    "    y_pred_bin = heaviside(y_pred)\n",
    "    print(\"x=\",x[k,:],\" y=\",y[k],\" y_pred_bin=\",y_pred_bin,\" y_pred=\",y_pred,sep=\"\")\n",
    "    c_pred = color[y_pred_bin]\n",
    "    c_true = color[y[k]]\n",
    "    plt.scatter([x[k,0]],[x[k,1]],color=c_pred,marker='.',edgecolors=c_true,linewidth=1,s=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
