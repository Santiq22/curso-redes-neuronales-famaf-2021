{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refs.\n",
    "\n",
    "1. Introduction to the theory of neuronal computation, Hertz, Krogh, Palmer (1991)\n",
    "\n",
    "2. https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/\n",
    "\n",
    "3. https://towardsdatascience.com/perceptron-explanation-implementation-and-a-visual-example-3c8e76b4e2d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teoría\n",
    "\n",
    "### Múltiples neuronas de salida\n",
    "\n",
    "Consideramos un perceptrón simple (una capa) de $n$ entradas y $m$ neuronas de salida.\n",
    "El vector $x\\in\\mathbb{R}^n$ representa el estado de las neuronas de entrada, y el vector $y\\in\\mathbb{R}^m$ el estado de las neuronas de salida.\n",
    "El estado de la $j$-ésima neurona de salida viene determinado por\n",
    "\n",
    "$$ g(b_j+w_jx) \\;\\;\\; (1)$$\n",
    "\n",
    "donde la constante $b_j\\in \\mathbb{R}$ se llama umbral, y el vector $w_j\\in \\mathbb{R}^n$ representa pesos sinápticos.\n",
    "Más precisamente, $b$ es un vector de umbrales y $w\\in\\mathbb{R}^{m,n}$ es una matriz de pesos sinápticos, donde\n",
    "\n",
    "$$ w_jx = \\Sigma_{i=1}^n w_{ji}x_i $$ \n",
    "\n",
    "es un producto escalar, $w_j$ es la $j$-esima fila de $w$, y $g\\in (\\mathbb{R} \\to \\mathbb{R})$ es alguna función activación (i.e. una función no decreciente que poseen alguna no linealidad).\n",
    "Notar que todas las neuronas de salida usan la misma función activación $g$.\n",
    "Existen muchas funciones de activación, pero nos enfocaremos en usar\n",
    "\n",
    "\\begin{eqnarray}\n",
    "g(x)\n",
    "&:=& \\tanh(\\beta x) \\\\\n",
    "&=& \\frac{e^{\\beta x}-e^{-\\beta x}}{e^{\\beta x}+e^{-\\beta x}} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "la cual graficamos a continuación para diferentes valores de $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1.0\n",
    "def g(x):\n",
    "    return np.tanh(beta*x)\n",
    "\n",
    "x = np.linspace(-3,3,1000)\n",
    "for _beta in [0.1,1.0,10.0,100.0]:\n",
    "    beta = _beta\n",
    "    plt.plot(x,np.vectorize(g)(x),label='beta='+str(beta))\n",
    "    \n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"$g_{\\\\beta}$\")\n",
    "plt.legend()\n",
    "\n",
    "Esta $g$ es diferenciable en $\\mathbb{R}$ y satisface\n",
    "\n",
    "$$ g' = \\beta(1-g^2) $$\n",
    "\n",
    "lo cual, convenientemente, puede ser usado para ahorrar recursos de cómputo.\n",
    "A continuación graficamos ésta derivada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dg(x):\n",
    "    return beta*(1.0-g(x)**2)\n",
    "\n",
    "x = np.linspace(-3,3,1000)\n",
    "for _beta in [2**i for i in range(-2,3)]:\n",
    "    beta = _beta\n",
    "    plt.plot(x,np.vectorize(lambda x:dg(x)/beta)(x),label='beta='+str(beta))\n",
    "    \n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"$g^{'}/\\\\beta$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en donde vemos que se transforma en un pico cuyo máximo crece como $\\sim \\beta$ en torno a $x=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **truco** útil consiste en notar que \n",
    "\n",
    "$$ b_j + w_jx = b_j + \\sum_{i=0}^n w_{ji}x_i $$ \n",
    "\n",
    "puede remplazarse por \n",
    "\n",
    "$$ w_jx = w_{j0}x_0 + \\sum_{i=1}^n w_{ji}x_i = \\sum_{i=0}^n w_{ji}x_i $$ \n",
    "\n",
    "si introducimos los pesos sinápticos $w_{j0}=b_j$ y una $0$-ésima neurona artificial de entrada de estado permanente $x_0=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento\n",
    "\n",
    "Consideremos una serie de datos de entrada $x_1,...,x_q$ donde $x_k\\in \\mathbb{R}^{1+n}$ para todo $k=1,...,q$, y una serie de datos de salida $y_1,...,y_q$ donde $y_k\\in \\mathbb{R}^m$ para todo $k=1,...,q$.\n",
    "Estos datos podrían ser generados por experimentos o sintéticamente, recordando que hay que fijar $x_{k0}=1$ para todo $k$ para poder aprovechar el **truquito** anteriormente mencionado.\n",
    "\n",
    "El objetivo es entrenar los pesos sinápticos de la red hasta que ésta logre aproximar, de la mejor manera posible, la relación entre los datos de entrada y los de salida.\n",
    "Formalmente, buscamos minimizar la suma de errores cuadráticos\n",
    "\n",
    "\\begin{eqnarray}\n",
    "e&:=&\\sum_{k=1}^q \\sum_{j=1}^m (y_{kj}-g(w_jx_k))^2 \\\\\n",
    "&=&\\sum_{k=1}^q \\sum_{j=1}^m (y_{kj}-g(z_{kj}))^2 \\\\\n",
    "&=&\\sum_{k=1}^q \\sum_{j=1}^m (y_{kj}-v_{kj})^2 \\\\\n",
    "&=&\\sum_{k=1}^q (y_k-v_k)^2 \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "con respecto a $w$, donde, en la última linea, $(y_k-v_k)^2$ representa el producto escalar del vector $y_k-v_k$ consigo mismo.\n",
    "Además, reconocemos aquí que los datos de salida constituyen una matriz $y\\in \\mathbb{R}^{q,m}$.\n",
    "Además, hemos introducido el $k$-ésimo vector de salida $v_k\\in \\mathbb{R}^m$, el cuál constituye la predicción que realiza la red neuronal cuando se la expone a la $k$-ésima entrada $x_k$, cuya $j$-ésima componente es\n",
    "\n",
    "$$ v_{kj} := g(z_{kj}) $$\n",
    "\n",
    "donde\n",
    "\n",
    "\\begin{eqnarray}\n",
    "z^T_{jk} & := & w_jx_k \\\\\n",
    "&=& \\sum_{i=0}^n w_{ji}x^T_{ik} \\\\\n",
    "&=& (wx^T)_{jk} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Si pensamos a $e$ como una función $e\\in (\\mathbb{R}^{m,(1+n)}\\ni w\\to \\mathbb{R})$, podemos intentar minimizarla descendiendo por el gradiente de componentes\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial e}{\\partial w_{ji}}\n",
    "&=& 2\\sum_{k=1}^q \\sum_{s=1}^m (y_{ks}-g(z_{ks}))\\frac{\\partial}{\\partial w_{ji}}(y_{ks}-g(z_{ks})) \\\\\n",
    "&=& -2\\sum_{k=1}^q \\sum_{s=1}^m (y_{ks}-g(z_{ks}))g'(z_{ks})\\frac{\\partial}{\\partial w_{ji}}z_{ks} \\\\\n",
    "&=& -2\\sum_{k=1}^q \\sum_{s=1}^m (y_{ks}-g(z_{ks}))g'(z_{ks})\\frac{\\partial}{\\partial w_{ji}} \\sum_{r=0}^n w_{sr}x_{kr} \\\\\n",
    "&=& 2\\sum_{k=1}^q \\sum_{s=1}^m (g(z_{ks})-y_{ks})g'(z_{ks}) \\sum_{r=0}^n x_{kr} \\delta_{ir} \\delta_{js} \\\\\n",
    "&=& 2\\sum_{k=1}^q \\sum_{s=1}^m (g(z_{ks})-y_{ks})g'(z_{ks}) x_{ki} \\delta_{js} \\\\\n",
    "&=& 2\\sum_{k=1}^q (g(z_{kj})-y_{kj})g'(z_{kj}) x_{ki}  \\\\\n",
    "&=& 2\\sum_{k=1}^q (v_{kj}-y_{kj})u_{kj} x_{ki} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "donde $x_{ki}$ es la $i$-ésima componente de la $k$-ésima muestra de entrada $x_k$.\n",
    "Además, por cuestiones prácticas, en la última línea hemos introducido el $k$-ésimo vector $u_k\\in \\mathbb{R}^m$ de $j$-ésima componente\n",
    "\n",
    "$$ u_{kj} := g'(z_{kj}) $$\n",
    "\n",
    "Para minimizar $e$ con respecto a $w$, consideramos un algoritmo iterativo en donde $w^{(0)},w^{(1)},...,w^{(t)},w^{(t+1)},...$ denotan los valores de $w$ en cada paso de iteración.\n",
    "Un sencillo algoritmo de minimización por el gradiente consiste en inicializar $w^{(0)}$ con valores elegidos al azar de alguna distribución centrada en 0, para luego ir actualizandolos iterativamente según las regla\n",
    "\n",
    "$$ w^{(t+1)}_{ji} := w^{(t)}_{ji} - r \\left.\\frac{\\partial e}{\\partial w_{ji}}\\right|_{w^{(t)}} $$\n",
    "\n",
    "donde $r>0$ es algún valor pequeño que determina la tasa de convergencia.\n",
    "La regla debe aplicarse para todo $i$ en cada iteración, hasta que veamos que el error $e$ deja de decrecer, estacionandose en algún valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minipráctico\n",
    "\n",
    "**a)** Usar `scikit-learn.datasets.make_classification` para crear un dataset para clasificación con:\n",
    "\n",
    "- 2 características (features)\n",
    "- 2 clases\n",
    "- 200 muestras\n",
    "- sin redundancia\n",
    "- 1 grupo (cluster) por clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "m = 2\n",
    "q = 100 # k=0,1,...,q-1 donde q = número de muestras.\n",
    "\n",
    "#num_classes = 2**m # hipercubo de 2^m vértices, o número binario de m dígitos.\n",
    "num_classes = 2 # hipercubo de 2^m vértices, o número binario de m dígitos.\n",
    "\n",
    "x_ki,y_k = make_classification(\n",
    "    n_features=n,\n",
    "    n_classes=num_classes,\n",
    "    n_samples=q,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ki.shape,y_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_ki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desdoblamos el índice y_k del 2^m clusters en la señal binaria de m neuronas de salida.\n",
    "# Más precisamente, hacemos\n",
    "#   cluster | número binario\n",
    "#       y_k | y_kj = b_2 b_1\n",
    "#         0 |          0   0\n",
    "#         1 |          0   1\n",
    "#         2 |          1   0\n",
    "#         3 |          1   1\n",
    "# donde b_j es el j-ésimo dígito menos significativo del numero binario b = b_2 b_1.\n",
    "\n",
    "def binario_de_m_digitos(y,m):\n",
    "    \"\"\"\n",
    "    Entrada \n",
    "        i : entero\n",
    "    Salida\n",
    "        b : binario de m digitos correspondiente a entero i.\n",
    "    \"\"\"\n",
    "    return [float(n) for n in bin(y)[2:].zfill(m)]\n",
    "\n",
    "y_kj = np.zeros((q,m))\n",
    "for k in range(q):\n",
    "    y_kj[k,:] = binario_de_m_digitos(y_k[k],m)\n",
    "#y_kj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Grafique el dataset generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = {0:'red',1:'blue',2:'green',3:'cyan'}\n",
    "for k in range(q):\n",
    "    plt.scatter([x_ki[k,0]],[x_ki[k,1]],c=color[y_k[k]])\n",
    "plt.title(\"datos\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Implemente un perceptron simple de $2$ neuronas de entrada y una de salida, utilizando\n",
    "\n",
    "$$ g(x) = \\tanh(\\beta x) $$ \n",
    "\n",
    "como función activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos la columna truquito\n",
    "tmp = np.ones((q,1+n))\n",
    "tmp[:,1:1+n] = x_ki\n",
    "x_ki = tmp\n",
    "#x_ki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(w_ji,x_i):\n",
    "    return np.vectorize(g)(np.dot(w_ji,x_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Implemente una funcion que entrene el perceptron. Utilice\n",
    "\n",
    "- $\\beta=1$\n",
    "- $r=0.01$\n",
    "- y $t_{max}=100$ iteraciones\n",
    "\n",
    "El entrenador debe tomar como entrada, entre otras cosas,\n",
    "\n",
    "- el dataset de entrenamiento, i.e. los vectores $x$,$y$, de componentes $x_{ki}$ e $y_k$ correspondiendo a la muestra $k$-ésima y la neurona de entrada $i$-ésima.\n",
    "- el vector de pesos sinápticos $w$\n",
    "\n",
    "y debe retornar un vector de componentes $e_t =$ que indiquen el error cuadrático tras $t$ iteraciones.\n",
    "Durante el proceso de entrenamiento, el entrenador debe modificar el vector $w$ que es pasado como argumento.\n",
    "\n",
    "Grafique $e_t$ vs $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar(w_ji,x_ki,y_k,r,tmax,g):\n",
    "    for t in range(tmax):\n",
    "        z_kj = np.dot(w_ji,x_ki.T).T\n",
    "        v_kj = np.vectorize(g)(z_kj)\n",
    "        u_kj = beta*(1-v_kj**2)\n",
    "        e_kj = (v_kj-y_kj)\n",
    "        eu_kj = e_kj*u_kj\n",
    "        w_ji -= 2*r*np.dot(x_ki.T,eu_kj).T\n",
    "        #e = np.linalg.norm(e_kj)        \n",
    "        e = np.dot(e_kj.flatten(),e_kj.flatten())        \n",
    "        yield t,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos pesos sinápticos\n",
    "w_ji = np.random.normal(size=(m,1+n))\n",
    "w_ji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=1.0\n",
    "r=0.01\n",
    "t_max=500\n",
    "\n",
    "tvec,evec=[],[]\n",
    "for t,e in entrenar(w_ji,x_ki,y_kj,r,t_max,g):\n",
    "    tvec.append(t)\n",
    "    evec.append(e)\n",
    "plt.plot(tvec,evec)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Grafique el resultado del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_a_binary(x):\n",
    "    if x>0.5:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def salida_a_integer(y):\n",
    "    b = np.vectorize(float_a_binary)(y)\n",
    "    return int('0b'+''.join([str(d) for d in b]),2)\n",
    "\n",
    "# Tiene que retornar el entero asociado a 011 que es 3.\n",
    "salida_a_integer([0.1,0.9,0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploteamos el resultado del entrenamiento\n",
    "for k in range(q):\n",
    "    c = color[salida_a_integer(perceptron(w_ji,x_ki[k,:]))]\n",
    "    plt.scatter([x_ki[k,1]],[x_ki[k,2]],c=c)\n",
    "\n",
    "# Agregamos linea de separacion\n",
    "xmin = np.min(x_ki[:,1])\n",
    "xmax = np.max(x_ki[:,1])\n",
    "xs = np.linspace(xmin,xmax,100)\n",
    "ymax = np.max(x_ki[:,2])\n",
    "ymin = np.min(x_ki[:,2])\n",
    "plt.title(\"prediccion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** Repita todo lo anterior probando con otras funciones de activación, valores de $r$ y de $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g)** Repita todo lo anterior usando un mapeo diferente entre las clases generadas con `scikit-learn` y la señal de salida a aprender. \n",
    "Más precisamente, el mapeo por el cual $y_{kj}=\\delta_{jc}$ si la $k$-ésima entrada $x_k$ corresponde a la clase $c$.\n",
    "Aquí, $\\delta_{jc}=1$ si $j=c$ y $\\delta_{jc}=0$ en caso contrario (esta delta se llama, la delta de Kronecker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
