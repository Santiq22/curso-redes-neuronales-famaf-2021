{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial de PyTorch 2\n",
    "\n",
    "## Instalación\n",
    "\n",
    "Instalaremos PyTorch usando conda y sin soporte para CUDA.\n",
    "Abrir una terminal o cónsola de bash.\n",
    "\n",
    "1. Activar conda o miniconda con Python 3\n",
    "\n",
    "        > source ~/miniconda3/bin/activate\n",
    "        \n",
    "2. Arear un nuevo environment en conda y activarlo\n",
    "\n",
    "        > conda create --name env-pytorch\n",
    "        > conda activate env-pytorch\n",
    "        \n",
    "3. Instalar PyTorch. Para ello, ir a la página\n",
    "\n",
    "    https://pytorch.org/get-started/locally/#start-locally\n",
    "    \n",
    "   y elegir:\n",
    "\n",
    "        PyTorch build      -> Stable\n",
    "        Your OS            -> Linux\n",
    "        Package            -> Conda\n",
    "        Language           -> Python\n",
    "        Compute platform   -> CPU\n",
    "    \n",
    "   Esto generará un comando debajo que deberemos ingresar en el environment de conda recientemente activado\n",
    "\n",
    "        (env-pytorch)> conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "        \n",
    "   Notar que, además de instalar pytorch, también instala torchvision y torchaudio. Estos paquetes incluyen, entre otras cosas, datasets con los cuales experimentar.\n",
    "\n",
    "4. Para completar el environment, instalar numpy, scipy, scikit-learn, jupyter, matplotlib y pandas\n",
    "        \n",
    "        (env-pytorch)> conda install -c anaconda numpy \n",
    "        (env-pytorch)> conda install -c conda install -c anaconda scipy \n",
    "        (env-pytorch)> conda install -c anaconda scikit-learn \n",
    "        (env-pytorch)> conda install -c conda install -c anaconda jupyter\n",
    "        (env-pytorch)> conda install -c conda-forge matplotlib\n",
    "        (env-pytorch)> conda install -c anaconda pandas\n",
    "        \n",
    "5. Opcionalmente, instalar PyTorchViz usando pip\n",
    "\n",
    "        (env-pytorch)> conda install -c conda-forge python-graphviz\n",
    "        (env-pytorch)> pip install torchviz\n",
    "        \n",
    "## Referencias\n",
    "\n",
    "    https://pytorch.org/tutorials/beginner/basics/intro.html\n",
    "    https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "    https://wiki.pathmind.com/comparison-frameworks-dl4j-tensorflow-pytorch#tensorflow\n",
    "    https://blog.paperspace.com/ultimate-guide-to-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testeamos la instalación\n",
    "\n",
    "Nos basamos en\n",
    "\n",
    "    https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "from torchviz import make_dot\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobre el uso de tensores de PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos crear tensores a partir de listas anidadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... o a partir de numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...o crearlos con valores predefinidos, respetando la forma y tipo de otros tensores prexistentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "x_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "x_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordar que shape determina el número y tamaño de las dimensiones del tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como con los arrays de numpy, además de shape y dtype (data type) los tensores de PyTorch tienen el atributo device, el cual especifica en que dispositivo (i.e. en que CPU o GPU) está instanciado el tensor.\n",
    "Es decir, este atributo es relevante sólo cuando trabajamos con GPUs o en un cluster de computadoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operaciones con tensores.\n",
    "Podemos mover o copiar un tensor desde la CPU a la GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En PyTorch el manejo de las componentes de los tensores (escalares, vectoriales, o sub-tensoriales) es equivalente al de numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print('First row: ', tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[:, -1])\n",
    "print('Last column:', tensor[..., -1])\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos concatenar tensores (ver también `torch.stack` que trabaja de manera similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.cat([tensor, 2*tensor, 3*tensor], dim=0)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.cat([tensor, 2*tensor, 3*tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes, son diferentes formas de multiplicar matricialmente dos tensores.\n",
    "Aquí `y1`, `y2`, `y3` resultarán con el mismo valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = tensor @ tensor.T\n",
    "\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera similar, estas son diferentes formas de multiplicar punto a punto dos tensores. \n",
    "Aquí `z1`, `z2`, `z3` resultarán con el mismo valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = tensor * tensor\n",
    "\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos, por ejemplo, sumar todas las componentes de un tensor para generar un tensor de una única componente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = tensor.sum()\n",
    "print(agg,type(agg)) # Esto es un tensor de PyTorch de una sola componente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para acceder al valor numérico en formato Python de dicha componente, usamos el método `.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_item = agg.item()\n",
    "print(agg_item,type(agg_item)) # Esto es (algo así como) un número flotante de Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In place operations: las funciones miembreo de un tensor que terminan en el caracter `_` corresponden a operaciones que actuan *in place*, i.e. que modifican\n",
    "las componentes del tensor sin generar una copia.\n",
    "Por ejemplo, la siguiente expresión suma 5 a cada componente del tensor llamado `tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.add_(5)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede crear una *vista* (view) en formato numpy de un tensor de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(5)\n",
    "n = t.numpy()\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que `n` no es una copia del contenido de `t`, sinó un view.\n",
    "Entonces, un cambio en el tensor `t` se ve reflejado en `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo y entrenando una red neuronal (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1: modelo simple, con numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos interesa aprender un simple modelo de regresión lineal\n",
    "\n",
    "$$ y = a + bx $$\n",
    "\n",
    "Comenzamos por generar los datos.\n",
    "Son $n=100$ puntos, en donde los *features* vienen representados por algunos valores $x_1,x_2,...$ de`x` y los *labels* por correspondientes valores $y_1,y_2,...$ de `y` dados por\n",
    "\n",
    "$$ y_i = a + bx_i + \\epsilon_i $$\n",
    "\n",
    "donde $a=1$, $b=2$ y $\\epsilon_i \\sim \\mathcal{N}(0,1)$ es un número aleatorio generado de una distribución gaussia de media 0 y varianza 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1) # features\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1) # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de aleatorizar el orden de los puntos, dividimos los datos en un conjunto de entrenamiento (de tamaño 80) y uno de validación (de tamaño 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffles the indices\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[80:]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(1,2)\n",
    "fig.set_size_inches(5.0,3.0)\n",
    "ax = axes[0]\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"training\")\n",
    "ax.scatter(x_train,y_train)\n",
    "ax = axes[1]\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"validation\")\n",
    "ax.scatter(val_idx,val_idx,c='r')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo último es aprender los valores de los parámetros $a$ y $b$ usados para generar los datos.\n",
    "Para ello consideramos la función de pérdida definida por el Error Cuadrático Medio entre las predicciones $\\hat{y}_i:=y(x_i)$ y los valores labels $y_i$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "L(x,y;a,b) &=& \\frac{1}{n}\\sum_{i=1}^n (y_i-y(x_i))^2 \\\\\n",
    "&=& \\frac{1}{n}\\sum_{i=1}^n (y_i-a-bx_i)^2 \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Usaremos el método del descenso por el gradiente\n",
    "\n",
    "$$ \\frac{\\partial L^{(t)}}{\\partial a}(x,y;a^{(t)},b^{(t)}) = -\\frac{2}{n}\\sum_{i=1}^n(y_i-y_i(x_i)) $$\n",
    "\n",
    "$$ \\frac{\\partial L^{(t)}}{\\partial b}(x,y;a^{(t)},b^{(t)}) = -\\frac{2}{n}\\sum_{i=1}^nx_i(y_i-y_i(x_i)) $$\n",
    "\n",
    "arrancando con valores iniciales de los parámetros $a=a^{(0)}$ y $b=b^{(0)}$ elegidos al azar de alguna distribución de probabilidades, para luego iterar sobre la época de entrenamiento $t$ según\n",
    "\n",
    "$$ a^{(t+1)} = a^{(t)} - \\eta \\frac{\\partial L^{(t)}}{\\partial a}(x,y;a^{(t)},b^{(t)}) $$\n",
    "\n",
    "$$ b^{(t+1)} = b^{(t)} - \\eta \\frac{\\partial L^{(t)}}{\\partial b}(x,y;a^{(t)},b^{(t)}) $$\n",
    "\n",
    "donde el $\\eta$ tal que $0<\\eta\\ll 1$, es la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes parameters \"a\" and \"b\" randomly\n",
    "np.random.seed(42)\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "print(f\"Iniciales a={a},b={b}\")\n",
    "\n",
    "# Sets learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Computes our model's predicted output\n",
    "    yhat = a + b * x_train\n",
    "    \n",
    "    # How wrong is our model? That's the error! \n",
    "    error = (y_train - yhat)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # Computes gradients for both \"a\" and \"b\" parameters\n",
    "    a_grad = -2 * error.mean()\n",
    "    b_grad = -2 * (x_train * error).mean()\n",
    "    \n",
    "    # Updates parameters using gradients and the learning rate\n",
    "    a = a - lr * a_grad\n",
    "    b = b - lr * b_grad\n",
    "\n",
    "print(f\"Ajustados a={a},b={b}\")\n",
    "\n",
    "# Sanity Check: do we get the same results as our gradient descent?\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(f\"Check     a={linr.intercept_},b={linr.coef_[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2: idem, pero con PyTorch.\n",
    "Convenientemente, PyTorch provee de `torch.autograd`, un motor de diferenciación automática para calcular gradientes.\n",
    "\n",
    "Para ello, en este ejemplo tenemos que decirle a torch que `a` y `b` son tensores que requieren gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "# Initializes parameters \"a\" and \"b\" randomly, ALMOST as we did in Numpy\n",
    "# since we want to apply gradient descent on these parameters, we need\n",
    "# to set REQUIRES_GRAD = TRUE\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(f\"Iniciales a={a.item()},b={b.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenemos el modelo con un optimizador personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float) #, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float) #, device=device)\n",
    "print(f\"Iniciales a={a.item()},b={b.item()}\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # No more manual computation of gradients! \n",
    "    # a_grad = -2 * error.mean()\n",
    "    # b_grad = -2 * (x_tensor * error).mean()\n",
    "    \n",
    "    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
    "    loss.backward()\n",
    "    # Let's check the computed gradients...\n",
    "    print(f\"epoch={epoch} a.grad={a.grad},b.grad={b.grad}\")    \n",
    "    \n",
    "    # We need to use NO_GRAD to keep the update out of the gradient computation\n",
    "    # Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses...\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "    \n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "print(f\"Ajustados a={a.item()},b={b.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que hemos usado `with torch.no_grad()` al momento de actualizar los valores de los parámetros `a` y `b`.\n",
    "\n",
    "Esto es así, porque de otra manera deberíamos cambiar el valor de `a.grad` y `b.grad` al actualizar los valores de `a` y `b`.\n",
    "Por alguna razón, a PyTorch no le gusta esto (quizás para evitar evalauciones ciruclares) y, por ende, si querémos actualizar los valores de `a` y `b`, tenemos que decirle primero a PyTorch que desactive la actualización automática de los gradientes.\n",
    "\n",
    "PyTorch calcula los gradientes de forma acumulativa.\n",
    "Por ello, cada vez que usamos los gradientes para actualizar los parámetros, luego debemos resetear a 0 los gradientes usando, por ejemplo, el método `.zero()`.\n",
    "\n",
    "En PyTorch los gradientes son actualizados tal como es especificado por el grafo computacional determinado por el modelo que uno define\n",
    "\n",
    "![](comp-graph-ej1.png \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A estos gráficos de arriba se los puede generar con torchviz\n",
    "#make_dot(yhat)\n",
    "#make_dot(error)\n",
    "#make_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 3: entrenando modelos con el optimizador proveido por PyTorch.\n",
    "\n",
    "Además del cálculo automático de gradientes, PyTorch provee de un optimizador que se encarga de la actualización automática de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float) #, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float) #, device=device)\n",
    "print(f\"Iniciales a={a.item()},b={b.item()}\")\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    loss.backward()    \n",
    "    \n",
    "    # No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    #     a -= lr * a.grad\n",
    "    #     b -= lr * b.grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No more telling PyTorch to let gradients go!\n",
    "    # a.grad.zero_()\n",
    "    # b.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(f\"Ajustados a={a.item()},b={b.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 4:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero chequeamos si hay disponibles placas GPU con CUDA, o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Usando el dispositivo {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, definimos nuestro modelo como una clase derivada de la clase `nn.Model`.\n",
    "Inicializamos la red neuronal usando el metodo`.__init__()`.\n",
    "Toda subclase de `nn.Module` implementa el como operar sobre datos de entrada (input) para generar una salida (output) a travéz del método `.forward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Primero convertimos los datos de entrada a un gran vector.\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Luego generamos una secuencia de transformaciones que implementan las capas de la red.\n",
    "        # Algunas capas consisten en transformaciones lineales, que incluyen parámetros.\n",
    "        # Otras consisten en funciones de activación no lineales, que no incluyen parámetros.\n",
    "        # Los tamaños de las capas se especifican a travez de los tamaños de las transformaciones lineales.\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Este método es usado internamente para transformar inputs en outputs en las fases forward.\n",
    "        # Al igual que antes, aplanamos los datos de entrada en un gran vector.\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego creamos una un objeto (o instancia) de la clase `NeuralNetwork`, y lo movemos al dispositivo que usaremos.\n",
    "Además, imprimimos su estructura para ver si todo salió como esperábamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar el modelo, le pasamos datos de entrada.\n",
    "Esto ejecuta el método `.forward()`, junto con algunas otras operaciones de fondo.\n",
    "Ojo! No hay que llamar `model.forward()` directamente!\n",
    "\n",
    "Tras llamar el modelo sobre algunos inputs, este retorna un tensor 10-dimensional con algunos valores crudos o *raw* predecidos para cada clase.\n",
    "Obtenemos de estos probabilidades de predicción por medio de pasarlos a travéz de una instancia del modulo `nn.Softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos datos de entrada truchos (una imagen de píxeles aleatorios).\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "\n",
    "# Se los damos de comer al modelo via el método .__call__() del mismo.\n",
    "# Notar que este modelo no está entrenado, por lo que el output será cualquier cosa.\n",
    "logits = model(X)\n",
    "\n",
    "# Imprimimos la salida cruda (raw) generado por el modelo\n",
    "print(f\"Salida del modelo = {logits}\")\n",
    "print(\"\")\n",
    "\n",
    "# Convertimos estos valores crudos a probabilidades y las imprimimos\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print(f\"Probabilidades predecidas = {pred_probab}\")\n",
    "print(\"\")\n",
    "\n",
    "# Elegimos la categoría asociada a la mayor probabilidad.\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Categoría predecida: {y_pred}\")\n",
    "\n",
    "#### Desglocemos el modelo\n",
    "\n",
    "# Desglocemos lo que ocurre en las capas del modelo de FashionMNIST.\n",
    "# Para ellos, tomaremos una muestra minilote de 3 imagenes de tamaño 28x28,\n",
    "# y veremos que ocurre si se lo pasamos a la red.\n",
    "\n",
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())\n",
    "\n",
    "# Inicializamos la capa nn.Flatten para convertir cada imagen 2D de 28x28 pixeles\n",
    "# a un array de 28*x28=748 valores contiguos (la dimensión de minibatch (dim=0) \n",
    "# se mantiene).\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "\n",
    "# Una capa lineal es un modulo que aplica una transformación lineal a la entrada,\n",
    "# usando los pesos (weights) y sesgos (biases) almacenados.\n",
    "\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())\n",
    "\n",
    "# Las unidades no lineales de activación sirven para lograr mapeos complejos entre\n",
    "# inputs y outputs.\n",
    "# Estas se aplican después de transformaciones lineales.\n",
    "# En este ejemplo, usamos nn.ReLUs entre las capas lineales.\n",
    "\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")\n",
    "\n",
    "# nn.Sequential is contenedor ordenado de módulos.\n",
    "# Los datos de entrada son pasados a travéz de los módulos en el mismo orden\n",
    "# en que fueron definidos.\n",
    "# Uno puede usar contenedores secuenciales para rápidamente crear una red como \n",
    "# seq_modules.\n",
    "\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "\n",
    "# La ultima capa de la red neuronal retorna procesando los datos con la función logit,\n",
    "# pasándolos por el módulo nn.Softmax\n",
    "# La función logit rescalea valores en [-infty,infty] a valores en [0,1]\n",
    "# para que estos valores representen probabilidades de predicción para cada clase.\n",
    "# El parámetro indica la dimensión sobre la cual los valores deben sumar 1.\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "\n",
    "# Los parámetros del modelo\n",
    "\n",
    "# Varias de las capas de una red neuronal están parametrizadas, i.e. poseen \n",
    "# pesos (weights) y tendencias (biases) que son optimizados durante el entrenamiento.\n",
    "# Al subclasear nn.Module, automaticamente tenemos en cuenta todos lso campos\n",
    "# definidos dentro del objeto *model*, y hace accessibles todos los parametros\n",
    "# allí definidos por medio de los métodos parameters() o named_parameters().\n",
    "# En este ejemplo, iteramos sobre todos los parámetros e imprimimos sus tamaños\n",
    "# y un preview de sus valores.\n",
    "\n",
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
