{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYDllb_5lU5g"
   },
   "source": [
    "# Conjuntos de datos de entrenamiento, validación y prueba\n",
    "\n",
    "**Refs**\n",
    "\n",
    "https://www.geeksforgeeks.org/training-neural-networks-with-validation-using-pytorch/\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o12jz3RVlU5j"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "#from torchviz import make_dot\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import dill\n",
    "import json\n",
    "import datetime\n",
    "try:\n",
    "  import google.colab\n",
    "  from google.colab import files  \n",
    "  COLAB = True\n",
    "except:\n",
    "  COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IguB0Rq0lU5l",
    "outputId": "e1789122-7b78-4622-b076-34c710086be8"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Usando el dispositivo {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxLTpiFHlU5l"
   },
   "source": [
    "Consideremos una familia de redes neuronales $y = f_a(x;w_a)$ de distintas arquitecturas $a$.\n",
    "Aquí, $x$ denota la entrada (ej. features) de la red, $y$ la salida (ej. labels) y $w_a$ los parámetros o pesos sinápticos de la misma.\n",
    "\n",
    "Redes de distintas arquitecturas pueden pueden aprender datasets de distintas estructuras.\n",
    "En particular, redes de distintas arquitecturas pueden tener distintos números de parámetros y por ende pueden aprender datasets de distintas complejidades.\n",
    "\n",
    "Sabemos que redes demasiado simples (con pocos parámetros) no logran aprender datasets suficientemente complejos, y que redes demasiado complejas tienden a sobrefitear datos.\n",
    "Entonces, nos interesa encontrar aquella red de la familia, que mejor se desempeñe con los datos a disposición, aprendiendo correctamente los datos de entrenamiento, pero también generalizando sin sobrefitear sobre datos de validación.\n",
    "Para ello, dividimos el conjunto de datos a disposición (el cuál se supone estar compuesto de muestras generadas de manera estadísticamente independiente) en tres conjuntos:\n",
    "\n",
    "1. el conjunto de entrenamiento (training),\n",
    "\n",
    "2. el conjunto de validación (validation), y\n",
    "\n",
    "3. el conjunto de testeo (test).\n",
    "\n",
    "Luego, para encontrar la red de arquitectura más conveniente, realizamos el siguiente procedimiento para cada arquitectura $a$:\n",
    "\n",
    "1. Entrenamos la red $f_a(x,w_a)$ optimizando con respecto a $w_a$ sobre las muestras $x$ obtenidas de dataset de entrenamiento, y utilizando la función de pérdida de nuestra preferencia. Esto resulta en valores \"optimos\" de los parámetros $\\hat{w}_a$, de manera que $f_a(x,\\hat{w}_a)$ constituye la red entrenada.\n",
    "\n",
    "2. Luego, usando métricas de nuestra preferencia (ej. la función de pérdida), evaluamos $f_a(x,\\hat{w}_a)$ sobre el conjunto de validación, para ver cuán bien generaliza la red ya entrenada.\n",
    "\n",
    "Luego, elegimos la arquitectura $\\hat{a}$ que haya dado los mejores resultados durante el paso de validación 2.\n",
    "Finalmente, caracterizamos las bondades de nuestra elección $f_{\\hat{a}}(x;w_{\\hat{a}})$ evaluándola sobre el conjunto de prueba (test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oU1MAg7lU5m"
   },
   "source": [
    "Veamos un ejemplo con FashionMNIST y una red multicapa de sólo una capa oculta.\n",
    "Para ello, comenzamos por crear los conjuntos de entrenamiento, validación y testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsnKTx1klU5m"
   },
   "outputs": [],
   "source": [
    "# La primera vez esto tarda un rato ya que tiene que bajar los datos de la red.\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X31MLlGClU5n"
   },
   "source": [
    "Luego definimos la red neuronal.\n",
    "Esta es un perceptron con una capa oculta de tamaño arbitrario $n$. \n",
    "En este ejemplo, dicho $n$ es el único grado de libertad que dejamos variar, de entre todos los que definen la arquitectura de la red.\n",
    "En otras palabras, nuestra familia estará compuesta de redes con capas ocultas de distintos tamaños."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohAkHpT3lU5o"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,n=128):\n",
    "        super(Net,self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(28*28,n)\n",
    "        self.linear2 = nn.Linear(n,10)\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKJWEsg0lU5o"
   },
   "source": [
    "Implementamos las funciones para entrenar, validar y testear un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tr22fl_alU5o"
   },
   "outputs": [],
   "source": [
    "# Definimos la función de entrenamiento\n",
    "def train_loop(dataloader,model,loss_fn,optimizer,verbose_each=32):  \n",
    "    # Calculamos cosas utiles que necesitamos\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    # Seteamos el modelo en modo entrenamiento. Esto sirve para activar, por ejemplo, dropout, etc. durante la fase de entrenamiento.\n",
    "    model.train()\n",
    "    # Pasamos el modelo la GPU si está disponible.        \n",
    "    model = model.to(device)    \n",
    "    # Iteramos sobre lotes (batchs)\n",
    "    for batch,(X,y) in enumerate(dataloader):\n",
    "        # Pasamos los tensores a la GPU si está disponible.\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)      \n",
    "        # Calculamos la predicción del modelo y la correspondiente pérdida (error)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "        # Backpropagamos usando el optimizador proveido.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Imprimimos el progreso cada 100 batchs\n",
    "        if batch % verbose_each*len(X) == 0:\n",
    "            loss   = loss.item()\n",
    "            sample = batch*len(X) # Número de batch * número de muestras en cada batch\n",
    "            #print(f\"batch={batch} loss={loss:>7f}  muestras-procesadas:[{sample:>5d}/{num_samples:>5d}]\")            \n",
    "# De manera similar, definimos la función de validación y testeo\n",
    "def test_loop(dataloader,model,loss_fn):\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    avrg_loss    = 0\n",
    "    frac_correct = 0\n",
    "    # Seteamos el modelo en modo evaluacion. Esto sirve para desactivar, por ejemplo, dropout, etc. cuando no estamos en una fase de entrenamiento.\n",
    "    model.eval()\n",
    "    # Pasamos el modelo la GPU si está disponible.    \n",
    "    model = model.to(device)    \n",
    "    # Para validar, desactivamos el cálculo de gradientes.\n",
    "    with torch.no_grad():\n",
    "        # Iteramos sobre lotes (batches)\n",
    "        for X,y in dataloader:\n",
    "            # Pasamos los tensores a la GPU si está disponible.\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)           \n",
    "            # Calculamos las predicciones del modelo...\n",
    "            pred = model(X)\n",
    "            # y las correspondientes pérdidas (errores), los cuales vamos acumulando en un valor total.\n",
    "            avrg_loss += loss_fn(pred,y).item()\n",
    "            # También calculamos el número de predicciones correctas, y lo acumulamos en un total.\n",
    "            frac_correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    # Calculamos la pérdida total y la fracción de clasificaciones correctas, y las imprimimos.\n",
    "    avrg_loss    /= num_batches\n",
    "    frac_correct /= num_samples\n",
    "    #print(f\"Test Error: \\n Accuracy: {frac_correct:>0.5f}, Avg. loss: {avrg_loss:>8f} \\n\")\n",
    "    return avrg_loss,frac_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0xr_QrL5lU5p",
    "outputId": "34119ade-7eac-4117-89ea-ba80de85b0b6"
   },
   "outputs": [],
   "source": [
    "# Definimos hiperparámetros de entrenamiento\n",
    "learning_rate = 1e-3\n",
    "batch_size = 500\n",
    "num_epochs = 100\n",
    "num_k = 1 #72\n",
    "n=2048\n",
    "# Creamos una funcion de perdida\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Creamos un DataFrame de pandas para ir almacenando los valores calculados.\n",
    "df = pd.DataFrame()\n",
    "# Simulamos por tramos porque google colab se desconecta antes de que concluya para todos los valores de n en la lista.\n",
    "# Recordar que 28*28=784\n",
    "for k in range(num_k):\n",
    "    # Creamos el modelo y el optimzador\n",
    "    model = Net(n)\n",
    "    # Creamos los dataloaders ...\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=batch_size)\n",
    "    # ... en particular, usamos el dataset de prueba (test) como dataset de validación\n",
    "    valid_dataloader = DataLoader(test_dataset,batch_size=batch_size)         \n",
    "    #optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "    # Entrenamos el modelo y calcualmos curvas.\n",
    "    min_valid_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loop(train_dataloader,model,loss_fn,optimizer)\n",
    "        train_loss,train_accu = test_loop(train_dataloader,model,loss_fn)\n",
    "        valid_loss,valid_accu = test_loop(valid_dataloader,model,loss_fn)\n",
    "        print(f\"n={n} k={k} epoch={epoch} train_loss={train_loss} train_accu={train_accu} valid_loss={valid_loss} valid_accu={valid_accu}\")\n",
    "        df = df.append({\"n\":n,\n",
    "                        \"k\":k,\n",
    "                        \"epoch\":epoch,\n",
    "                        \"train_loss\":train_loss,\n",
    "                        \"train_accu\":train_accu,\n",
    "                        \"valid_loss\":valid_loss,\n",
    "                        \"valid_accu\":valid_accu}\n",
    "                        ,ignore_index=True)\n",
    "json_fname = \"simulation-results-\"+datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")+\".json\"\n",
    "df.to_json(json_fname)\n",
    "if COLAB:\n",
    "    files.download(json_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmTLVXzxqMaA"
   },
   "source": [
    "**Simulation summary**\n",
    "\n",
    "simulation-results-2021-12-15-18-37-04.json\n",
    "\n",
    "simulation-results-2021-12-15-18-59-55.json\n",
    "\n",
    "simulation-results-2021-12-15-19-18-31.json\n",
    "\n",
    "simulation-results-2021-12-15-19-42-48.json\n",
    "\n",
    "simulation-results-2021-12-15-20-39-16.json\n",
    "\n",
    "simulation-results-2021-12-15-21-10-38.json\n",
    "\n",
    "simulation-results-2021-12-15-22-38-52.json\n",
    "\n",
    "simulation-results-2021-12-15-23-46-54.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIU3e2dklU5p"
   },
   "outputs": [],
   "source": [
    "#df = pd.read_json(json_fname)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tot7HU2hfEU_"
   },
   "outputs": [],
   "source": [
    "%%bash --out list_json\n",
    "# Usamos el bash magic de Jupyter para ver que archivos *.json hemos creado.\n",
    "# Guardamos el resultado en la variable list_json\n",
    "ls *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kj0LW5SgfSm6",
    "outputId": "ded5f758-b852-4e44-a21b-b7a75c0a86e0"
   },
   "outputs": [],
   "source": [
    "list_json = list_json.split()\n",
    "list_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "rdDy24Mfe0Ns",
    "outputId": "3d4ecaf7-ad69-4617-b37d-99e99ab89be8"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_json(json_fname) for json_fname in list_json],ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "mF_TjZJXlU5q",
    "outputId": "924adce0-6f38-4b64-fc81-bb69203fdd86"
   },
   "outputs": [],
   "source": [
    "df1 = df.drop(\"k\",1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "_qIvbMdQlU5q",
    "outputId": "173292b2-e5d0-4b06-9050-2f0fd049af3c"
   },
   "outputs": [],
   "source": [
    "df2 = df1.pivot_table(index=[\"n\",\"epoch\"],aggfunc=\"count\").reset_index()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "MOr6ZZpMn4Yr",
    "outputId": "6a1630b5-76ef-4f58-e0d7-05a0d21341b5"
   },
   "outputs": [],
   "source": [
    "df3 = df1.pivot_table(index=[\"n\",\"epoch\"],aggfunc=\"mean\").reset_index()\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUZGqbG_lU5q"
   },
   "source": [
    "Visualicemos el desempeño de la red, graficando la loss y la accuracy vs el número de épocas de entrenamiento para los distintos tamaños de la capa oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "zOrtRFX_lU5q",
    "outputId": "e04280cb-8ccc-4d2c-c6a8-111fda2bab2e"
   },
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(1,2)\n",
    "fig.set_size_inches(10.0,5.0)\n",
    "colors = cm.Dark2.colors\n",
    "for color,n in zip(colors,df[\"n\"].unique()):\n",
    "    dfn = df3[df3[\"n\"]==n]\n",
    "    x = dfn[\"epoch\"]\n",
    "    ax = axes[0]\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    ax.plot(x,dfn[\"train_loss\"],label=f\"train n={n}\",color=color)\n",
    "    ax.plot(x,dfn[\"valid_loss\"],label=f\"valid n={n}\",color=color,linestyle='--')\n",
    "    ax.legend()\n",
    "    ax = axes[1]\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"accuracy\")\n",
    "    ax.plot(x,dfn[\"train_accu\"],label=f\"train n={n}\",color=color)\n",
    "    ax.plot(x,dfn[\"valid_accu\"],label=f\"test n={n}\",color=color,linestyle='--')\n",
    "    ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Graficamos la min. loss y la max. accuracy en función del tamaño de la capa oculta. El min. loss y el max. accuracy se computan sobre las correspondientes curvas en función del número de épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "J7Aoa9qslU5r",
    "outputId": "fadc487a-0faf-4f52-80e2-a866c825cae0"
   },
   "outputs": [],
   "source": [
    "df4 = df3.pivot_table(index=[\"n\"],\n",
    "                    aggfunc={\n",
    "                        \"train_loss\":min,\n",
    "                        \"valid_loss\":min,\n",
    "                        \"train_accu\":max,\n",
    "                        \"valid_accu\":max,\n",
    "                    }\n",
    "                   ).reset_index()\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "MhJsGcB7p9rL",
    "outputId": "5e1cfa82-550e-4902-fc7f-c071cec97e41"
   },
   "outputs": [],
   "source": [
    "x=df4[\"n\"]\n",
    "fig,axes=plt.subplots(1,2)\n",
    "fig.set_size_inches(10.0,5.0)\n",
    "ax = axes[0]\n",
    "ax.set_xlabel(\"n\")\n",
    "ax.set_ylabel(\"min. loss\")\n",
    "ax.scatter(x,df4[\"train_loss\"],label=f\"train\")\n",
    "ax.plot(x,df4[\"train_loss\"],label=f\"train\")\n",
    "ax.scatter(x,df4[\"valid_loss\"],label=f\"valid\",linestyle='--')\n",
    "ax.plot(x,df4[\"valid_loss\"],label=f\"valid\",linestyle='--')\n",
    "ax.set_xscale(\"log\")\n",
    "#ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "ax = axes[1]\n",
    "ax.set_xlabel(\"n\")\n",
    "ax.set_ylabel(\"max. accuracy\")\n",
    "ax.scatter(x,df4[\"train_accu\"],label=f\"train\")\n",
    "ax.plot(x,df4[\"train_accu\"],label=f\"train\")\n",
    "ax.scatter(x,df4[\"valid_accu\"],label=f\"valid\",linestyle='--')\n",
    "ax.plot(x,df4[\"valid_accu\"],label=f\"valid\",linestyle='--')\n",
    "ax.set_xscale(\"log\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()    "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "collapsed_sections": [],
   "name": "entrenamiento-validacion-prueba.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
