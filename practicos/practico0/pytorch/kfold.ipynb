{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FsC_h24oGPe"
   },
   "source": [
    "# Datos de entrenamiento, validación y prueba, y el método de **k-fold** de validación cruzada\n",
    "\n",
    "https://es.wikipedia.org/wiki/Validaci%C3%B3n_cruzada\n",
    "\n",
    "https://www.machinecurve.com/index.php/2021/02/03/how-to-use-k-fold-cross-validation-with-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSvBckh6n_tS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "#from torchviz import make_dot\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import dill\n",
    "import json\n",
    "import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import files  \n",
    "    COLAB = True\n",
    "except:\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sgy3mevK25K4",
    "outputId": "713a0ba3-fe1f-4b66-899c-a8e93d2be263"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Usando el dispositivo {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmRiFv1xo5tN"
   },
   "outputs": [],
   "source": [
    "# La primera vez esto tarda un rato ya que tiene que bajar los datos de la red.\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCVwK-lEyR-L"
   },
   "source": [
    "Definimos el modelo de red neuronal. \n",
    "En este caso, una red convolucional profunda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wU4qqEOAWsh"
   },
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        ## Primera capa convolucional:\n",
    "        ## construimos 32 canales usando filtros (kernels) de 3x3\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=32,            \n",
    "                kernel_size=3,                                 \n",
    "                padding=1,                  \n",
    "            ),\n",
    "            ## Aplicamos Batch Normalization como regularización\n",
    "            nn.BatchNorm2d(32),  \n",
    "            ## Aplicamos la función de activación               \n",
    "            nn.ReLU(),    \n",
    "            ## Reducimos la imagen con Max Pooling                  \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),    \n",
    "        )\n",
    "\n",
    "        ## Segunda capa convolucional:\n",
    "        ## construimos 64 canales usando filtros (kernels) de 3x3\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=32,              \n",
    "                out_channels=64,            \n",
    "                kernel_size=3,                               \n",
    "                padding=0\n",
    "            ),    \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                \n",
    "        )\n",
    "\n",
    "        ## \"Achatamos\" la salida de la última capa, de 64 canales \n",
    "        ## de tamaño 6x6, transformandola en un vector de 64*6*6 elementos\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        ## Después de las capas convoulucionales,\n",
    "        ## agregamos algunas capas densas. La última, de 10\n",
    "        ## neurnonas, es nuestra capa de salida\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(64 * 6 * 6, 600),\n",
    "            nn.Dropout(0.25), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 120),\n",
    "            nn.Dropout(0.25), ## Regularizamos con dropout después de cada capa\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 10),\n",
    "            nn.Dropout(0.25), \n",
    "            nn.ReLU() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)     \n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKR6DjcswGPJ"
   },
   "outputs": [],
   "source": [
    "# Definimos la función de entrenamiento\n",
    "def train_loop(dataloader,model,loss_fn,optimizer,verbose_each=32):  \n",
    "    # Calculamos cosas utiles que necesitamos\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    # Seteamos el modelo en modo entrenamiento. Esto sirve para activar, por ejemplo, dropout, etc. durante la fase de entrenamiento.\n",
    "    model.train()\n",
    "    # Pasamos el modelo la GPU si está disponible.        \n",
    "    model = model.to(device)    \n",
    "    # Iteramos sobre lotes (batchs)\n",
    "    for batch,(X,y) in enumerate(dataloader):\n",
    "        # Pasamos los tensores a la GPU si está disponible.\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)      \n",
    "        # Calculamos la predicción del modelo y la correspondiente pérdida (error)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "        # Backpropagamos usando el optimizador proveido.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Imprimimos el progreso cada 100 batchs\n",
    "        if batch % verbose_each*len(X) == 0:\n",
    "            loss   = loss.item()\n",
    "            sample = batch*len(X) # Número de batch * número de muestras en cada batch\n",
    "            #print(f\"batch={batch} loss={loss:>7f}  muestras-procesadas:[{sample:>5d}/{num_samples:>5d}]\")            \n",
    "# De manera similar, definimos la función de validación y testeo\n",
    "def test_loop(dataloader,model,loss_fn):\n",
    "    num_samples  = 0\n",
    "    num_batches  = 0\n",
    "    avrg_loss    = 0\n",
    "    frac_correct = 0\n",
    "    # Seteamos el modelo en modo evaluacion. Esto sirve para desactivar, por ejemplo, dropout, etc. cuando no estamos en una fase de entrenamiento.\n",
    "    model.eval()\n",
    "    # Pasamos el modelo la GPU si está disponible.    \n",
    "    model = model.to(device)    \n",
    "    # Para validar, desactivamos el cálculo de gradientes.\n",
    "    with torch.no_grad():\n",
    "        # Iteramos sobre lotes (batches)\n",
    "        for X,y in dataloader:\n",
    "            # Pasamos los tensores a la GPU si está disponible.\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)           \n",
    "            # Calculamos las predicciones del modelo...\n",
    "            pred = model(X)\n",
    "            # y las correspondientes pérdidas (errores), los cuales vamos acumulando en un valor total.\n",
    "            avrg_loss += loss_fn(pred,y).item()\n",
    "            # También calculamos el número de predicciones correctas, y lo acumulamos en un total.\n",
    "            num_batches += 1\n",
    "            num_samples += y.size(0)\n",
    "            frac_correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    # Calculamos la pérdida total y la fracción de clasificaciones correctas, y las imprimimos.\n",
    "    avrg_loss    /= num_batches\n",
    "    frac_correct /= num_samples\n",
    "    #print(f\"Test Error: \\n Accuracy: {frac_correct:>0.5f}, Avg. loss: {avrg_loss:>8f} \\n\")\n",
    "    return avrg_loss,frac_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcutTqQ002DE"
   },
   "source": [
    "Utilizando el método de K-fold, entrenamos y validamos modelos. En el proceso, vamos grabando los resultados en un dataframe de la librería pandas, y también, vamos grabando el modelo que mejor resultado de validación dió hasta el momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ynr6FnaWwLUK",
    "outputId": "23382c39-5da7-465d-f104-09f771a25548"
   },
   "outputs": [],
   "source": [
    "# Definimos hiperparámetros de entrenamiento\n",
    "init_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "learning_rate = 1e-3\n",
    "batch_size = 1000\n",
    "max_epochs = 25\n",
    "num_k = 6\n",
    "# Creamos un DataFrame de pandas para ir almacenando los valores calculados.\n",
    "df = pd.DataFrame()\n",
    "# Creamos una funcion de perdida\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Creamos el generador de k-folds\n",
    "kfold = KFold(n_splits=num_k,shuffle=False)\n",
    "# Simulamos por tramos porque google colab se desconecta antes de que concluya para todos los valores de n en la lista.\n",
    "min_valid_loss = 10000000.0\n",
    "max_valid_accu = 0.0\n",
    "for k,(train_ids,valid_ids) in enumerate(kfold.split(train_dataset)):\n",
    "    # Creamos los dataloaders de entrenamiento y validacion\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,sampler=train_subsampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,sampler=valid_subsampler)\n",
    "    # Creamos el modelo\n",
    "    model = ConvolutionalNeuralNet()\n",
    "    # Creamos el optimizador\n",
    "    #optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "    # Entrenamos el modelo, calcualmos y almacenamos valores de metricas obtenidos\n",
    "    min_valid_loss = float(\"inf\")\n",
    "    for epoch in range(max_epochs):\n",
    "        train_loop(train_loader,model,loss_fn,optimizer)\n",
    "        train_loss,train_accu = test_loop(train_loader,model,loss_fn)\n",
    "        valid_loss,valid_accu = test_loop(valid_loader,model,loss_fn)\n",
    "        print(f\"k={k} epoch={epoch} train_loss={train_loss} train_accu={train_accu} valid_loss={valid_loss} valid_accu={valid_accu}\")\n",
    "        df = df.append({\"k\":k,\n",
    "                        \"epoch\":epoch,\n",
    "                        \"train_loss\":train_loss,\n",
    "                        \"train_accu\":train_accu,\n",
    "                        \"valid_loss\":valid_loss,\n",
    "                        \"valid_accu\":valid_accu}\n",
    "                        ,ignore_index=True)\n",
    "        if min_valid_loss > valid_loss: # or max_valid_accu < valid_accu:\n",
    "            if min_valid_loss > valid_loss:\n",
    "                min_valid_loss = valid_loss\n",
    "            if max_valid_accu < valid_accu:\n",
    "                max_valid_accu = valid_accu\n",
    "            # Guardamos los parámetros del modelo.\n",
    "            model_fname = \"kfold-best-model-\"+init_datetime+\".ptm\"\n",
    "            print(\"   Saving model_fname =\",model_fname,end=\"\")\n",
    "            print(\" ... DONE!\")\n",
    "            torch.save(model.state_dict(),model_fname)\n",
    "json_fname = \"kfold-simulation-results-\"+init_datetime+\".json\"\n",
    "df.to_json(json_fname)\n",
    "if COLAB:\n",
    "    files.download(model_fname)\n",
    "    files.download(json_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbwvEMNppM56"
   },
   "source": [
    "Cargamos el dataframe de pandas con los resultados de los cómputos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Msc6H9rX3F5v"
   },
   "outputs": [],
   "source": [
    "%%bash --out list_json\n",
    "# Usamos el bash magic de Jupyter para ver que archivos *.json hemos creado.\n",
    "# Guardamos el resultado en la variable list_json\n",
    "ls *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QsQZC-PpKix",
    "outputId": "c18bd37d-fbbb-45c6-c4f9-b16dce4ddd03"
   },
   "outputs": [],
   "source": [
    "list_json = list_json.split()\n",
    "list_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "gzY6UpFtpLup",
    "outputId": "d4e3730f-9124-44a8-d163-30987b057213"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_json(json_fname) for json_fname in list_json],ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYo6aM0SpkS4"
   },
   "source": [
    "Procesamos los datos para poder graficarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "_Ng8Gl7zpYNN",
    "outputId": "9060cce9-c01b-4f16-c0ed-53297b460539"
   },
   "outputs": [],
   "source": [
    "# Eliminamos la columna \"k\" que indexa los folds.\n",
    "df1 = df.drop(\"k\",1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "DQk0jryfpcOX",
    "outputId": "f88fd4c5-75ce-442e-f7df-f2055e3efd3b"
   },
   "outputs": [],
   "source": [
    "# Contamos cuantas muestras hay en cada metrica por cada epoca\n",
    "df2 = df1.pivot_table(index=[\"epoch\"],aggfunc=\"count\").reset_index()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "1FuiVIc4pe9_",
    "outputId": "9a515b2f-4bf7-46a4-a3b2-48f122cfd478"
   },
   "outputs": [],
   "source": [
    "# Para epoca, calculamos el promedio de cada metrica sobre muestras\n",
    "df3 = df1.pivot_table(index=[\"epoch\"],aggfunc=\"mean\").reset_index()\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9jh_Sh3qCtP"
   },
   "source": [
    "Grafiquemos las metricas vs epocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "n_vysniDp_QN",
    "outputId": "e05a363e-5c9b-493f-9bf6-5108613078b3"
   },
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(1,2)\n",
    "fig.set_size_inches(10.0,5.0)\n",
    "colors = cm.Dark2.colors\n",
    "for color in colors[:1]:\n",
    "    x = df3[\"epoch\"]\n",
    "    ax = axes[0]\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    ax.plot(x,df3[\"train_loss\"],label=f\"train\",color=color)\n",
    "    ax.plot(x,df3[\"valid_loss\"],label=f\"valid\",color=color,linestyle='--')\n",
    "    ax.legend()\n",
    "    ax = axes[1]\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"accuracy\")\n",
    "    ax.plot(x,df3[\"train_accu\"],label=f\"train\",color=color)\n",
    "    ax.plot(x,df3[\"valid_accu\"],label=f\"test\",color=color,linestyle='--')\n",
    "    ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9aG1Eafq9AW"
   },
   "source": [
    "Vemos que en $\\mathsf{epoch} \\approx 15$, $\\mathsf{valid}$ $\\mathsf{loss}$ se estabilza en $\\approx 0.3$, mientras que $\\mathsf{train}$ $\\mathsf{loss}$ sigue bajando. Al mismo tiempo, vemos que $\\mathsf{train}$_$\\mathsf{accuracy}$ crece en todo el rango mientras que $\\mathsf{valid}$ $\\mathsf{accuracy}$ ya está estabilizado para $\\mathsf{epoch} ≳ 15$. Concluimos, entonces, que entrenar el modelo por $\\mathsf{epoch} \\approx 15$ épocas es lo recomendable. Por otro lado, es importante resaltar que en dicho punto el modelo tiende a sobrefitear los datos de entrenamiento ya que $(\\mathsf{valid}$ $\\mathsf{loss}$ - $\\mathsf{train}$ $\\mathsf{loss})$/$\\mathsf{train}$ $\\mathsf{loss}$ $\\approx 50\\%$. Esto quiere decir que el modelo es demasiado complejo.\n",
    "\n",
    "La última actualización del modelo óptimo corresponde a \n",
    "\n",
    "    k=5 epoch=13 train_loss=0.18559566855430604 train_accu=0.95972 valid_loss=0.28687887638807297 valid_accu=0.9231\n",
    "        Saving model_fname = kfold-best-model-2022-02-07-21-45-00.ptm ... DONE!\n",
    "\n",
    "ocurriendo para $\\mathsf{epoch}=13$, lo cual es muy cercano al valor $\\mathsf{epoch}=15$ que mencionamos anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQY2VfgyAz2a"
   },
   "source": [
    "Probamos el modelo óptimo seleccionado por el algoritmo (correspondiente a $\\mathsf{epoch}=13$) en el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NF9MYJCCAzS1"
   },
   "outputs": [],
   "source": [
    "%%bash --out model_fname\n",
    "# Usamos el bash magic de Jupyter para ver que archivos *.json hemos creado.\n",
    "# Guardamos el resultado en la variable list_json\n",
    "ls *.ptm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aEpocR2rutm"
   },
   "outputs": [],
   "source": [
    "model_fname = model_fname.split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKLYq3taCN61"
   },
   "outputs": [],
   "source": [
    "model = ConvolutionalNeuralNet()\n",
    "model.load_state_dict(torch.load(model_fname,map_location=\"cpu\"))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eMncdtePCjev",
    "outputId": "b4649e67-8cb0-4152-a335-77b71fec2e02"
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size)\n",
    "test_loss,test_accu = test_loop(test_loader,model,loss_fn)\n",
    "print(\"test_loss = \",test_loss)\n",
    "print(\"test_accu = \",test_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5dTIuptDg4h"
   },
   "source": [
    "Por comparación:\n",
    "\n",
    "    epoch\ttrain_accu\ttrain_loss\tvalid_accu\tvalid_loss\n",
    "    ...\n",
    "    12\t   0.946237\t   0.225827\t  0.915500\t  0.318581\n",
    "    13\t   0.954733\t   0.190428\t  0.919967\t  0.290598\n",
    "    14\t   0.957810\t   0.186052\t  0.920117\t  0.298289\n",
    "    15\t   0.958237\t   0.186528\t  0.918267\t  0.305730\n",
    "    16\t   0.963387\t   0.165904\t  0.922633\t  0.290206\n",
    "    17\t   0.965727\t   0.159663\t  0.922067\t  0.291162\n",
    "    ...\n",
    "\n",
    "Concluimos así que los valores de validación son confiables, y que $\\mathsf{epoch} = 15$ constituye un adecuado valor de número de épocas de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzrmPMZ7RD48"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "kfold.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
