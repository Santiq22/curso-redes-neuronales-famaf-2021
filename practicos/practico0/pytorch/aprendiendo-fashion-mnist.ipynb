{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer modelo \"serio\", aprendiendo Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "from torchviz import make_dot\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 4: \n",
    "\n",
    "Podemos pensar una red neuronal como un *módulo* compuesto de otros *módulos* (p. ej. las capas).\n",
    "El namespace (espacio de nombres) `torch.nn` provee de diversos building blocks (bloques de construcción) que facilitan la creación de redes neuronales de arquitecturas complejas por medio de la composición de módulos.\n",
    "Más precisamente, PyTorch provee la clase `nn.Module`, la cual podemos subclasear para definir módulos propios.\n",
    "\n",
    "Veamos un ejemplo de uso, en donde construiremos una red neuronal para clasificar imágenes del dataset FashionMNIST.\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero chequeamos si hay disponibles placas GPU con CUDA, o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Usando el dispositivo {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, definimos nuestro modelo como una clase derivada de la clase `nn.Module`.\n",
    "Inicializamos la red neuronal usando el metodo`.__init__()`.\n",
    "Toda subclase de `nn.Module` implementa el como operar sobre datos de entrada (input) para generar una salida (output) a travéz del método `.forward()`.\n",
    "Por otro lado, notar que NO implementamos el método `.backward()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Intermezzo.** Clases base y derivada. Las clases son el concepto central de la programación orientada a objetos.\n",
    "Las clases definen la interface entre  los objetos de una dada *clase* y el resto del programa.\n",
    "Esta inteface está compuesta por miembros.\n",
    "Hay, básicamente, dos tipos de miembros: los métodos (funciones) y los atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos una clase\n",
    "class A:\n",
    "    def __init__(self,nombre):\n",
    "        self.nombre = nombre\n",
    "    def __str__(self):\n",
    "        return f\"Nombre={self.nombre}\"\n",
    "    def uppercase(self):\n",
    "        return self.nombre.upper()\n",
    "    def say_hi(self):\n",
    "        print(\"Hi!\")\n",
    "\n",
    "# E instanciemos un objeto de la misma\n",
    "a = A(\"mi-objeto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspeccionemos su atributo .nombre\n",
    "a.nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoquemos su método .uppercase()\n",
    "a.uppercase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoquemos su método .say_hi()\n",
    "a.say_hi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoquemos el método .__str__()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos una clase derivada\n",
    "class Z(A):\n",
    "    def __init__(self,nombre,cantidad):\n",
    "        super(Z,self).__init__(nombre) # Llamemos al inicializador de la clase base (o super-clase).\n",
    "        self.cantidad=cantidad\n",
    "    def __str__(self):\n",
    "        return f\"Nombre={self.nombre}, Cantidad={self.cantidad}\"\n",
    "    def add_n(self,n):\n",
    "        return self.cantidad+n\n",
    "    def say_hi(self): # Redefinamos el método .say_hi()\n",
    "        print(\"Hello!\")\n",
    "    \n",
    "# Instanciemos un objeto de la clase Z\n",
    "z = Z(\"mi-otro-objeto\",10)\n",
    "# e invoquemos el método .__str__()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos el atributo .cantidad\n",
    "z.cantidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y también el miembro heredado .nombre\n",
    "z.nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoquemos el método .add1()\n",
    "z.add_n(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoquemos, también, el método .uppercase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.uppercase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y, finalmente, invoquemos el método heredado y redefinido .say_hi()\n",
    "z.say_hi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que los métodos (y atributos) heredados se comportantal como fueron definidos en la clase base (`A` en este caso), salvo que se los redefinan en la clase derivada (`Z` en este caso).\n",
    "\n",
    "Las clases bases son también conocidas como super-clases y las clases derivadas como sub-clases.\n",
    "Una clase puede tener varias super-clases.\n",
    "De una clase pueden derivarse varias sub-clases.\n",
    "\n",
    "Si una clase A tiene una super clase B, y B tiene una superclase C, entonces C es una superclase (no inmediata) de A.\n",
    "Decimos que existe una dependencia circular entre dos clases A y B, si A es una superclase de B y B una superclase de A.\n",
    "Hay que evitar dependencias circulares entre clases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Primero convertimos los datos de entrada a un gran vector.\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Luego generamos una secuencia de transformaciones que implementan las capas de la red.\n",
    "        # Algunas capas consisten en transformaciones lineales, que incluyen parámetros.\n",
    "        # Otras consisten en funciones de activación no lineales (en este caso de tipo ReLU), que no incluyen parámetros.\n",
    "        # Los tamaños de las capas se especifican a travez de los tamaños de las transformaciones lineales.\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Este método es usado internamente para transformar inputs en outputs en las fases forward.\n",
    "        # Al igual que antes, aplanamos los datos de entrada en un gran vector.\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Intermezzo.** Recordemos que es una *logit*. En estadística, la *logit* es la inversa de la función *logística*\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "donde $x\\in (-\\infty,\\infty)$ y $\\sigma(x)\\in (0,1)$. En particular, $\\sigma(x\\to -\\infty) = 0$ y $\\sigma(x\\to \\infty) = 1$.\n",
    "Por ende, \n",
    "\n",
    "$$ \\mathsf{logit}(p) = \\sigma^{-1}(p) = \\ln\\bigg(\\frac{p}{1-p}\\bigg) $$\n",
    "\n",
    "donde $p\\in (0,1)$ y $\\mathsf{logit}(p)\\in (-\\infty,\\infty)$. En particular, $\\mathsf{logit}(p\\to 0^+) \\to -\\infty$ y $\\mathsf{logit}(p\\to 1^-) \\to \\infty$.\n",
    "\n",
    "La *logística* es una función creciente que convierte un número real en un número entre 0 y 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego creamos una un objeto (o instancia) de la clase `NeuralNetwork`, y lo movemos al dispositivo que usaremos.\n",
    "Además, imprimimos su estructura para ver inspeccionar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Intermezzo** Recordemos que la función $\\mathsf{softmax} \\in \\mathbb{R}^n\\to \\mathbb{R}^n$ viene dada por\n",
    "\n",
    "$$ \\mathsf{softmax}_i(v) = \\frac{e^{v_i}}{\\sum_j e^{v_j}} $$\n",
    "\n",
    "donde $v=(v_1,v_2,...,v_n)$ y $v_i \\in (-\\infty,\\infty)$. \n",
    "Notar, $\\sum_i \\mathsf{softmax}_i(v) = 1$.\n",
    "La función $\\mathsf{softmax}$ provee una forma de convertir una tupla de números reales a una distribución de probabilidades acorde.\n",
    "\n",
    "---\n",
    "\n",
    "Para usar el modelo, le pasamos datos de entrada.\n",
    "Esto ejecuta el método `.forward()`, junto con algunas otras operaciones de fondo.\n",
    "Ojo! No hay que llamar `model.forward()` directamente!\n",
    "\n",
    "Tras llamar el modelo sobre algunos inputs, este retorna un tensor 10-dimensional con algunos valores crudos o *raw* predecidos para cada clase.\n",
    "Obtenemos de estos probabilidades de predicción por medio de pasarlos a travéz de una instancia del modulo `nn.Softmax`.\n",
    "\n",
    "Veamos un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos una muestra de entrada trucha que imite a FashionMNIST (una imagen de píxeles aleatorios).\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "\n",
    "# Se los damos de comer al modelo via el método .__call__() del mismo, que hereda de la clase nn.Module.\n",
    "# Eventualmente, el método .__call__() se encargará de llamar al método .forward() que nosotros hemos \n",
    "# implementado.\n",
    "# Notar que este modelo no está entrenado, por lo que el output será \"cualquier cosa\".\n",
    "logits = model(X)\n",
    "\n",
    "# Imprimimos la salida cruda (raw) generada por el modelo\n",
    "print(f\"Salida del modelo = {logits}\")\n",
    "print(\"\")\n",
    "\n",
    "# Usando la función \"softmax\", convertimos estos valores crudos a probabilidades y las imprimimos\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print(f\"Probabilidades predecidas = {pred_probab}\")\n",
    "print(\"\")\n",
    "\n",
    "# Elegimos la categoría asociada a la mayor probabilidad.\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Categoría predecida: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, veamos como entrenar el modelo con el dataset *FashionMNIST*.\n",
    "\n",
    "https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "Este dataset se implementa en términos de las clases `Dataset` y `DataLoader` proveidas por PyTorch.\n",
    "Respecitivamente, estas clases se usan para cargar y muestrear conjuntos de datos de formas convenientes y compatibles con PyTorch.\n",
    "\n",
    "Comenzamos por cargar el dataset de entrenamiento (train) y el dataset de prueba (test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La primera vez esto tarda un rato ya que tiene que bajar los datos de la red.\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender mejor que son estos objetos, veamos de que clases derivan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.__class__.__mro__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.__class__.__mro__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender como funciona la clase `Dataset`, veamos brevemente un ejemplo de una clase personalizada que se deriva de la misma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "# Por el momento, a esta clase la definimos pero no la usamos.\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    # Redefinimos el método .__len__()\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    # Redefinimos el método .__getitem__()    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploremos un poco los datos de FashionMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las muestras de FashionMNIST consisten en imágenes en 256 escalas de grises de 28x28 píxeles cada una.\n",
    "# Las imágenes constituyen prendas, calzados, etc.\n",
    "# Cada imagen tiene asociado un etiqueta (label) que puede adoptar un valor de entre 10 opciones.\n",
    "# Cada etiqueta representa una de las 10 categorías en las que las imágenes fueron clasificadas.\n",
    "# El objetivo es aprender a clasificar éstas imágenes de acuerdo a las etiquetas asociadas.\n",
    "\n",
    "# Las etiquetas son\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "# Cada dataset puede ser indexado.\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generemos ahora instancias de la clase `DataLoader` para estos objetos derivados de la clase `DataSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=9)\n",
    "test_dataloader = DataLoader(test_data, batch_size=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como estos objetos funcionan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un iterador a partir del objeto \"train_dataloader\"\n",
    "\n",
    "train_dataloader_iterable = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pedimos un batch\n",
    "\n",
    "train_features, train_labels = next(train_dataloader_iterable)\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "# Graficamos las imágenes del batch.\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(cols * rows):\n",
    "    img   = train_features[i]\n",
    "    label = train_labels[i]\n",
    "    figure.add_subplot(rows, cols, i+1)\n",
    "    plt.title(labels_map[label.item()])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    #print(f\"Label: {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya estamos en condiciones de implementar el entrenador del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función de entrenamiento\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # Iteramos sobre lotes (batchs)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Calculamos la predicción del modelo y la correspondiente pérdida (error)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagamos usando el optimizador proveido.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Imprimimos el progreso...\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"batch={batch} loss={loss:>7f}  muestras-procesadas:[{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# De manera similar, definimos la función de testeo\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Para testear, desactivamos el cálculo de gradientes.\n",
    "    with torch.no_grad():\n",
    "        # Iteramos sobre lotes (batches)\n",
    "        for X, y in dataloader:\n",
    "            # Calculamos las predicciones del modelo...\n",
    "            pred = model(X)\n",
    "            # y las correspondientes pérdidas (errores), los cuales vamos acumulando en un valor total.\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # También calculamos el número de predicciones correctas, y lo acumulamos en un total.\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # Calculamos la pérdida total y la fracción de clasificaciones correctas, y las imprimimos.\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los hiperparámetros del entrenamiento\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una instancia de una función de pérdida, una entropy loss en este caso\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# y un optimizador, un Stochastic Gradient Descent, en este caso.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Finalmente, entrenamos iterando sobre épocas\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
